---
title: '[ISL]CH4_Classification_Lab'
author: "Philip oh"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## 주식 시장 자료
- 이 자료는 2001년에서 2005년까지 1,250일에 걸친 S&P 500 주가지수의 수익률로 구성되어 있다.
- 이전 5일의 각 거래일 Lag1에서 Lag5에 대한 수익률이 기록되어 있다.
- 또한 Voulme은 전날에 거래된 주식수를 10억 주 단위로 표시한 것이고
- Today는 당일의 수익률,
- Direction은 당일 주가지수가 Up(상승)인지 Down(하락)인지를 나타낸다.

```{r message=F}
library(ISLR)
```

```{r}
names(Smarket)
summary(Smarket)
```

```{r}
cor(Smarket[,-9])
```


```{r}
attach(Smarket)
plot(Volume)
```

- Voume은 조금씩 증가하고 있다는 것을 알 수 있다. 즉, 일별 평균 거래 주식수는 2001년부터 2005년까지 꾸준히 증가해왔다.

## 로지스틱 회귀

```{r}
glm.fit = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
              data = Smarket,
              family = binomial)
summary(glm.fit)
```

- 전체적으로 p값이 높다.
- p값이 가장 낮은 것은 Lag1이다.
- 설명변수의 계수가 음수인 것은 어제의 수익률이 양수이면 오늘 주가지수가 하락할 가능성이 높다는 것이다.
- 하지만, 0.145의 p값은 여전히 큰 값이므로 Lag1과 Direction 사이에도 실질적인 상관성이 있다는 명백한 증거는 없다.

```{r}
glm.probs = predict(glm.fit, type = "response")
head(glm.probs, 10)
```

- `predict` 함수를 통해 주어진 설명변수 값에 대해 주가지수가 상승할 확률을 예측할 수 있다.

```{r}
glm.pred = rep("Down", 1250)
glm.pred[glm.probs>.5] = "Up"
```

- 먼저, 1,250개의 Down 원소로 구성된 벡터를 생성한다.
- 그리고 만약 glm.probs가 0.5를 초과한다면, glm.pred의 값을 "up"으로 값으로 바꾼다.

```{r}
table(glm.pred, Direction)
(507 + 145) / 1250
mean(glm.pred == Direction)
```

- `table` 함수를 사용하여 혼동행렬을 확인할 수 있다. 이는 얼마나 많은 관측치가 올바르게 또는 잘못 분류되었는지 확인할 수 있다.
- 혼동행렬에서 대각원소들은 올바른 예측을 나타내고 비대각원소들은 잘못된 예측을 나타낸다. 즉, 이 모델은 145일과 507일을 더해서 1250일 중 652일을 정확하게 예측했다.
- `mean` 함수를 통해 예측이 맞았던 날의 비율을 계산할 수 있다.

### training data와 testing data를 구분해야 하는 이유
- 위의 결과만 가지고, 해당 로지스틱 회귀모델이 임의 추측보다 낫다고 판단할 수도 있다. 하지만 이는 옳지 않다. 동일한 셋에 대해 모델을 training하고 testing 했기 때문이다. 일반적으로 훈련오차율은 지나치게 낙관적이다. 즉, 검정오차율을 과소평가하는 경향이 있따. 만약 이 모델을 다른 데이터셋에 적용한다면, 검정율과 오차율은 달라질 수 있다.
- 그래서 하는 것이 데이터셋을 training data와 testing data로 나누는 것이다.

### Training data와 Testing data의 분리
```{r}
train = (Year < 2005) # train 변수에 Year 값이 2005 미만인 것들만 할당한다.
# 이 변수에는 불린형 값이 할당된다.

Smarket.2005 = Smarket[!train, ] # Smarket 데이터셋에서 train 값이 False인 것들만 할당한다.
Direction.2005 = Direction[!train] # 마찬가지로, Direction에서 train 값이 False인 것들만 할당한다.
```

- 2005년 이전의 데이터를 training data로 하고 2005년의 데이터는 testing data로 구분하였다.

```{r}
glm.fit = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume,
              data = Smarket,
              family = binomial,
              subset = train)
glm.probs = predict(glm.fit, Smarket.2005, type = "response")
```

- 이제 2005년 이전의 관측치만 가지고 로지스틱 회귀모델을 적합하고 이 모델을 통해 2005년 이전의 각 날짜에 대해 주가지수가 상승할 확률을 얻는다.

```{r}
glm.pred = rep("Down",  252)
glm.pred[glm.probs>.5] = "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
mean(glm.pred != Direction.2005)
```

- training data로 적합한 모델을 testing data에 적용하여 예측해보았고, 검정오차율이 52%가 나왔다.
- 당연한 말이지만, training data와 testing data가 일치했던 이전의 모델보다 검정오차율이 높게 나왔다.
- 하지만, 일반적으로 전날의 수익률을 이용하여 미래의 주가지수의 움직임 방향을 예측하기 힘드므로 이 결과는 그리 놀랍지 않다.

### Lag1과 Lag2만을 사용하여 로지스틱 회귀모델 적합
```{r}
glm.fit = glm(Direction ~ Lag1 + Lag2, 
              data = Smarket,
              family = binomial,
              subset = train)
glm.probs = predict(glm.fit, Smarket.2005, type = "response")
glm.pred = rep("Down", 252)
glm.pred[glm.probs > .5] = "Up"
table(glm.pred, Direction.2005)
mean(glm.pred == Direction.2005)
```

- 일반적인 p값보다는 높지만, 그나마 다른 변수들보다 p값이 낮은 Lag1과 Lag2만을 사용하여 동일한 방법으로 로지스틱 회귀모델에 적합했다.
- 검정오차율은 44%로 줄어들었고, 일별 주가지수 움직임의 56%를 예측할 수 있었다.

# Lag1과 Lag2의 특정값에 연관된 수익률 예측
```{r}
predict(glm.fit, newdata = data.frame(Lag1 = c(1.2, 1.5), Lag2 = c(1.1, -0.8)),
        type = "response")
```


## 선형판별분석(LDA)

```{r message=F}
library(MASS)
```
```{r}
lda.fit = lda(Direction ~ Lag1 + Lag2, data = Smarket, subset = train)
lda.fit
```

```{r}
lda.pred = predict(lda.fit, Smarket.2005)
names(lda.pred)
```

- `predict` 함수는 3개의 원소를 가진 리스트를 반환한다.
- 첫 번째 원소인 class는 주가지수의 움직임에 대한 LDA의 예측을 포함한다.
- 두 번째 원소인 posterior(사후확률)는 k번째 열은 대응하는 관측치가 k번째 클래스에 속하는 사후확률이다.
- 세 번째 원소인 x는 선형판별이다.

```{r}
lda.class = lda.pred$class
table(lda.class, Direction.2005)
mean(lda.class == Direction.2005)
```

